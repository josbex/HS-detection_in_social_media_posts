{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hate_speech_detection_in_social_media_posts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/x6txdlD0E8EgPPhcRFy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06fbcd17ff8e4d6282d9721a721a2bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7984dafa37bd4fc3b57e5513ac94292b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c2e6c030fd04bb3b379326facdd4feb",
              "IPY_MODEL_d98baecd08c84d9f9124f1d3facd7a03"
            ]
          }
        },
        "7984dafa37bd4fc3b57e5513ac94292b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c2e6c030fd04bb3b379326facdd4feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_feee6e1d7bf54635b6e6e37db2f5a9c8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78209bb27d0a4207b7d900ab1972a647"
          }
        },
        "d98baecd08c84d9f9124f1d3facd7a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ae9200bdd524c04934c971ad420c912",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 665kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5db589b707ab4dfc9cf5b01926980529"
          }
        },
        "feee6e1d7bf54635b6e6e37db2f5a9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78209bb27d0a4207b7d900ab1972a647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ae9200bdd524c04934c971ad420c912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5db589b707ab4dfc9cf5b01926980529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45b5df7fe1534f16bb09bfd23f5fd463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1bc3989d58b34ee9a337af4d24f2ed4f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7be968f6943747c6b3f665393c17475b",
              "IPY_MODEL_3cc77f3edd774f0ba5296e20d13b90e1"
            ]
          }
        },
        "1bc3989d58b34ee9a337af4d24f2ed4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7be968f6943747c6b3f665393c17475b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43c7acecb56e480d8caa0b5fda3a5861",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6699b47f1a124319a88ba73deae1aff0"
          }
        },
        "3cc77f3edd774f0ba5296e20d13b90e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f381d6626b8b47c29bc917ad27f46963",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.67kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5a7a6737ecf45da8e792c706038ab2e"
          }
        },
        "43c7acecb56e480d8caa0b5fda3a5861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6699b47f1a124319a88ba73deae1aff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f381d6626b8b47c29bc917ad27f46963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5a7a6737ecf45da8e792c706038ab2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e292e28de0648998e88701946a7a662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b4431cec230e4993bbffdd1c430e0d3b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9f666e36622a41f1a8e43b0871f62f03",
              "IPY_MODEL_97979412cc944b449923c696819cd5b6"
            ]
          }
        },
        "b4431cec230e4993bbffdd1c430e0d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f666e36622a41f1a8e43b0871f62f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d79f9010e47440ef917a2fc2d8350172",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d57ef2afba8c4340801f73bff6882738"
          }
        },
        "97979412cc944b449923c696819cd5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2472a285dea7401784dc820c58321d31",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [01:46&lt;00:00, 4.14MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b3044f70d65420394fe30da0972f25b"
          }
        },
        "d79f9010e47440ef917a2fc2d8350172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d57ef2afba8c4340801f73bff6882738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2472a285dea7401784dc820c58321d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b3044f70d65420394fe30da0972f25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josbex/HS-detection_in_social_media_posts/blob/master/Hate_speech_detection_in_social_media_posts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebRJLK5pFEIW",
        "colab_type": "text"
      },
      "source": [
        "#Hate Speech detection in social media posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZixYXbbewRl",
        "colab_type": "text"
      },
      "source": [
        "## Using GPU for training BERT model\n",
        "\n",
        "Go to: Edit -> Notebook settinngs -> Hardware accelerator -> (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMLxwf4dLCT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as  pd\n",
        "import re \n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnchHIGhe2IV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6286e5c5-7110-4a01-bd54-14c6a5bc7273"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oerwfJaHfO-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a79c3e6b-da57-4d1d-d0d5-77bcb273bcb2"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpprsVDOFzee",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset needs to be loaded from the drive (this entails the dataset is in your drive). If so, just run the cell below and follow the link to get an authorization code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShEoz1diF3Wq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "57b507ba-3e2b-40ec-ab72-b0762db83a12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHePnpgILWI2",
        "colab_type": "text"
      },
      "source": [
        "After the the dataset can be read from the drive, just specify the name of the dataset you want to read. For this case the OLID training dataset is loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4WblfTmLo3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "4589ff23-d8c8-46be-d4fe-f19709d9d23a"
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/olid-training-v1.0.tsv\", sep=\"\\t\") \n",
        "print(df.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ftF8nAF4Qb",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing \n",
        "\n",
        "Some steps are needed for the data processing first the tweet-preprocesser (https://pypi.org/project/tweet-preprocessor/) is used for cleaning up the tweets from urls, users, hashtags and emoticons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rz_BFbXF7xH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "ed932dc4-7b2c-4bdb-b186-802a8eb499cd"
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpY3j120lcfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessor as p"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAC9wwE_HFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_tweets():\n",
        "  p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI)\n",
        "  for tweet in df.tweet:\n",
        "    df.replace(tweet, p.tokenize(tweet), inplace=True)\n",
        "\n",
        "def remove_pattern(input_txt, pattern, replace):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, replace, input_txt)    \n",
        "    return input_txt  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V30IH1HWiZ7z",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize tweets\n",
        "\n",
        "Here we replace the @, emojis and urls using the tweet preprocessor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gm-Yu1OAsw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8a3155c-47b3-4143-81ca-74e3b36e66b4"
      },
      "source": [
        "tokenize_tweets()\n",
        "print(df.tweet[1]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$MENTION$ $MENTION$ Go home youre drunk!!! $MENTION$ #MAGA #Trump2020 $EMOJI$$EMOJI$ URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOgCkkgLKVEt",
        "colab_type": "text"
      },
      "source": [
        "## Vocab\n",
        "\n",
        "To be able to parse hashtags efficiently a big vocabulary is needed and just for this method the vocab needs to be in a list format sorted in word lenght order, shortest to longest. This will be updated later to be saved to an csv file, just to make it easier to add new words. \n",
        "\n",
        "As of now a list of 3000 most common english words was combined with a list of 1300 differents slurs and curse words. We can probably add some better vocab list later since this one doesn't work for different variations of words, for example it can split #humanright but not #humanrights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSim2kOrKXRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['a', 'I', 'ad', 'ah', 'AM', 'as', 'at', 'be', 'by', 'do', 'go', 'he', 'hi', 'ie', 'if', 'in', 'it', 'me', 'Mr', 'Ms', 'my', 'no', 'of', 'oh', 'ok', 'on', 'or', 'PC', 'PM', 'so', 'to', 'TV', 'up', 'us', 'vs', 'we', 'bi', 'fu', 'ho', 'ky', 'uk', 'wn', 'act', 'add', 'age', 'ago', 'aid', 'aim', 'air', 'all', 'and', 'any', 'arm', 'art', 'ask', 'bad', 'bag', 'ban', 'bar', 'bed', 'bet', 'big', 'bit', 'box', 'boy', 'bus', 'but', 'buy', 'can', 'cap', 'car', 'cat', 'CEO', 'cop', 'cow', 'cry', 'cup', 'cut', 'dad', 'day', 'die', 'dig', 'DNA', 'dog', 'dry', 'due', 'ear', 'eat', 'egg', 'end', 'era', 'etc', 'eye', 'fan', 'far', 'fat', 'fee', 'few', 'fit', 'fix', 'fly', 'for', 'fun', 'gap', 'gas', 'gay', 'get', 'God', 'gun', 'guy', 'hat', 'her', 'hey', 'him', 'hip', 'his', 'hit', 'hot', 'how', 'ice', 'ill', 'its', 'jet', 'Jew', 'job', 'joy', 'key', 'kid', 'lab', 'lap', 'law', 'lay', 'leg', 'let', 'lie', 'lip', 'lot', 'low', 'mad', 'man', 'map', 'may', 'mix', 'mom', 'Mrs', 'net', 'new', 'nod', 'nor', 'not', 'now', \"n't\", 'nut', 'odd', 'off', 'oil', 'old', 'one', 'our', 'out', 'owe', 'own', 'pan', 'pay', 'per', 'pet', 'pie', 'pop', 'pot', 'put', 'raw', 'red', 'rid', 'row', 'rub', 'run', 'sad', 'say', 'sea', 'see', 'set', 'sex', 'she', 'sin', 'sir', 'sit', 'six', 'ski', 'sky', 'son', 'sue', 'sun', 'tap', 'tax', 'tea', 'ten', 'the', 'tie', 'tip', \n",
        "'toe', 'too', 'top', 'toy', 'try', 'two', 'use', 'via', 'war', 'way', 'wet', 'who', 'why', 'win', 'yes', 'yet', 'you', 'abo', 'ass', 'bra', 'cum', 'die', 'dix', 'ero', 'evl', 'fag', 'fat', 'fok', 'fuc', 'fuk', 'gay', 'gin', 'gob', 'god', 'goy', 'gun', 'gyp', 'hiv', 'jap', 'jew', 'kid', 'kkk', 'kum', 'lez', 'lsd', 'mad', 'nig', 'nip', 'pee', 'pom', 'poo', 'pot', 'pud', 'sex', 'sob', 'sos', 'tit', 'tnt', 'uck', 'wab', 'wog', 'wop', 'wtf', 'xtc', 'xxx', 'able', 'acid', 'aide', 'AIDS', 'ally', 'also', 'Arab', 'area', 'army', 'auto', 'away', 'baby', 'back', 'bake', 'ball', 'band', 'bank', 'base', 'bean', 'bear', 'beat', 'beer', 'bell', 'belt', 'bend', 'best', 'bike', 'bill', 'bind', 'bird', 'bite', 'blow', 'blue', 'boat', 'body', 'bomb', 'bond', 'bone', 'book', 'boom', 'boot', 'born', 'boss', 'both', 'bowl', 'buck', 'burn', 'bury', 'busy', 'cake', 'call', 'camp', 'card', 'care', 'case', 'cash', 'cast', 'cell', 'chef', 'chip', 'cite', 'city', 'club', 'clue', 'coal', 'coat', 'code', 'cold', 'come', 'cook', 'cool', 'cope', 'copy', 'core', 'corn', 'cost', 'crew', 'crop', 'dare', 'dark', 'data', 'date', 'dead', 'deal', 'dear', 'debt', 'deck', 'deep', 'deer', 'deny', 'desk', 'diet', 'dirt', 'dish', 'door', 'down', 'drag', 'draw', 'drop', 'drug', 'dust', 'duty', 'each', 'earn', 'ease', 'east', 'easy', 'edge', 'else', 'even', 'ever', 'face', 'fact', 'fade', 'fail', 'fair', 'fall', 'farm', 'fast', 'fate', 'fear', 'feed', 'feel', 'file', 'fill', 'film', 'find', 'fine', 'fire', 'firm', 'fish', 'five', 'flag', 'flat', 'flee', 'flow', 'folk', 'food', 'foot', 'form', \n",
        "'four', 'free', 'from', 'fuel', 'full', 'fund', 'gain', 'game', 'gang', 'gate', 'gaze', 'gear', 'gene', 'gift', 'girl', 'give', 'glad', 'goal', 'gold', 'golf', 'good', 'grab', 'gray', 'grow', 'hair', 'half', 'hall', 'hand', 'hang', 'hard', 'hate', 'have', 'head', 'hear', 'heat', 'heel', 'hell', 'help', 'here', 'hero', 'hide', 'high', 'hill', 'hire', 'hold', 'hole', 'holy', 'home', 'hope', 'host', 'hour', 'huge', 'hurt', 'idea', 'into', 'iron', 'item', 'jail', 'join', 'joke', 'jump', 'jury', 'just', 'keep', 'kick', 'kill', 'kind', 'king', 'kiss', 'knee', 'know', 'lack', 'lady', 'lake', 'land', 'last', 'late', 'lawn', 'lead', 'leaf', 'lean', 'left', 'less', 'life', 'lift', 'like', 'line', 'link', 'list', 'live', 'load', 'loan', 'lock', 'long', 'look', 'lose', 'loss', 'lost', 'lots', 'loud', 'love', 'luck', 'lung', 'mail', 'main', 'make', 'male', 'mall', 'many', 'mark', 'mask', 'mass', 'math', 'meal', 'mean', 'meat', 'meet', 'menu', 'mere', 'mess', 'milk', 'mind', 'mine', 'miss', 'mode', 'mood', 'moon', 'more', 'most', 'move', 'much', 'must', 'myth', 'name', 'near', 'neck', 'need', 'news', 'next', 'nice', 'nine', 'none', 'nose', 'note', 'odds', 'okay', 'once', 'only', 'onto', 'open', 'oven', 'over', 'pace', 'pack', 'page', 'pain', 'pair', 'pale', 'palm', 'pant', 'park', 'part', 'pass', 'past', 'path', 'peak', 'peer', 'pick', 'pile', 'pine', 'pink', 'pipe', 'plan', 'play', 'plot', 'plus', 'poem', 'poet', 'pole', 'poll', 'pool', 'poor', 'port', 'pose', 'post', 'pour', 'pray', 'pull', 'pure', 'push', 'quit', 'race', 'rail', 'rain', 'rank', 'rare', 'rate', 'read', 'real', 'rely', 'rest', 'rice', 'rich', 'ride', 'ring', 'rise', 'risk', 'road', 'rock', 'role', 'roll', 'roof', 'room', 'root', 'rope', 'rose', 'rule', 'rush', 'safe', 'sake', 'sale', 'salt', 'same', 'sand', 'save', \n",
        "'seat', 'seed', 'seek', 'seem', 'self', 'sell', 'send', 'ship', 'shit', 'shoe', 'shop', 'shot', 'show', 'shut', 'sick', 'side', 'sigh', 'sign', 'sing', 'sink', 'site', 'size', 'skin', 'slip', 'slow', 'snap', 'snow', 'soft', 'soil', 'some', 'song', 'soon', 'sort', 'soul', 'soup', 'spin', 'spot', 'star', 'stay', 'step', 'stir', 'stop', 'such', 'suit', 'sure', 'swim', 'tail', 'take', 'tale', 'talk', 'tall', 'tank', 'tape', 'task', 'team', 'tear', 'teen', 'tell', 'tend', 'tent', 'term', 'test', 'text', 'than', 'that', 'them', 'then', 'they', 'thin', 'this', 'thus', 'time', 'tiny', 'tire', 'tone', 'tool', 'toss', 'tour', 'town', 'tree', 'trip', 'true', 'tube', 'turn', 'twin', 'type', 'ugly', 'unit', 'upon', 'urge', 'used', 'user', 'vary', 'vast', 'very', 'view', 'vote', 'wage', 'wait', 'wake', 'walk', 'wall', 'want', 'warm', 'warn', 'wash', 'wave', 'weak', 'wear', 'week', 'well', 'west', 'what', 'when', 'whom', 'wide', 'wife', 'wild', 'will', 'wind', 'wine', 'wing', 'wipe', 'wire', 'wise', 'wish', 'with', 'wood', 'word', 'work', 'wrap', 'yard', 'yeah', 'year', 'yell', 'your', 'zone', 'abbo', 'anal', 'anus', 'arab', 'arse', 'babe', 'barf', 'bast', 'blow', 'bomb', 'bomd', 'bong', 'boob', 'boom', 'burn', 'butt', 'chav', 'chin', 'cigs', 'clit', 'cock', 'coon', 'crap', 'cumm', 'cunn', 'cunt', 'dago', 'damn', 'dead', 'dego', 'deth', 'dick', 'died', 'dies', 'dike', 'dink', 'dive', 'dong', 'doom', 'dope', 'drug', 'dumb', 'dyke', 'fart', 'fear', 'fire', 'floo', 'fore', 'fuck', 'fuks', 'geez', 'geni', 'gipp', 'gook', 'groe', 'gypo', 'gypp', 'hapa', 'hebe', 'heeb', 'hell', 'hobo', 'hoes', 'hole', 'homo', 'honk', 'hook', 'hore', 'hork', 'horn', 'ikey', 'itch', 'jade', 'jeez', 'jiga', 'jigg', 'jism', 'jiz', 'jizz', 'jugs', 'kike', 'kill', 'kink', 'kock', 'koon', 'krap', 'kums', 'kunt', 'kyke', 'laid', 'lezz', 'lies', 'limy', 'mams', 'meth', 'milf', 'mofo', 'moky', 'muff', 'munt', 'nazi', 'nigg', 'nigr', 'nook', 'nude', 'nuke', 'oral', 'orga', 'orgy', 'paki', 'payo', 'peck', 'perv', 'phuk', 'phuq', 'piky', 'pimp', 'piss', 'pixy', 'pohm', 'poon', 'poop', 'porn', 'pric', 'pros', 'pube', 'pudd', 'puke', 'puss', 'pusy', 'quim', 'rape', 'rere', 'rump', 'scag', 'scat', 'scum', 'sexy', 'shag', 'shat', 'shav', 'shit', 'sick', 'skum', 'slav', 'slut', 'smut', 'snot', 'spic', 'spig', 'spik', 'spit', 'suck', 'taff', 'tang', 'tard', 'teat', 'tits', 'turd', 'twat', 'vibr', 'wank', 'wetb', 'whit', 'whiz', 'whop', 'wuss', 'about', 'above', 'abuse', 'actor', 'adapt', 'admit', 'adopt', 'adult', 'after', 'again', 'agent', 'agree', 'ahead', 'album', 'alive', 'allow', 'alone', 'along', 'alter', 'among', 'anger', 'angle', 'angry', 'apart', 'apple', 'apply', 'argue', 'arise', 'armed', 'Asian', 'aside', 'asset', 'avoid', 'award', 'aware', 'awful', 'badly', 'basic', 'basis', 'beach', 'begin', 'being', 'below', 'bench', 'Bible', 'birth', 'black', 'blade', 'blame', 'blind', 'block', 'blood', 'board', 'brain', 'brand', 'bread', 'break', 'brick', 'brief', 'bring', 'broad', 'brown', 'brush', 'build', 'bunch', 'buyer', 'cabin', 'cable', 'carry', 'catch', 'cause', 'chain', 'chair', 'chart', 'chase', 'cheap', 'check', 'cheek', 'chest', 'chief', 'child', 'civil', 'claim', 'class', 'clean', 'clear', 'climb', 'clock', 'close', 'cloud', 'coach', 'coast', 'color', 'couch', 'could', 'count', 'court', 'cover', 'crack', 'craft', 'crash', 'crazy', 'cream', 'crime', 'cross', 'crowd', 'cycle', 'daily', 'dance', 'death', 'delay', 'depth', 'dirty', 'doubt', 'dozen', 'draft', 'drama', 'dream', 'dress', 'drink', 'drive', 'eager', 'early', 'earth', 'eight', 'elect', 'elite', 'empty', 'enemy', 'enjoy', 'enter', 'entry', 'equal', 'error', 'essay', 'event', 'every', 'exact', 'exist', 'extra', 'faith', 'false', 'fault', 'favor', 'fence', 'fewer', 'fiber', 'field', 'fifth', 'fifty', 'fight', 'final', 'first', 'flame', 'flesh', 'float', 'floor', 'focus', 'force', 'forth', 'found', 'frame', 'fresh', 'front', 'fruit', 'fully', 'funny', 'ghost', 'giant', 'given', 'glass', 'glove', 'grade', 'grain', 'grand', 'grant', 'grass', 'grave', 'great', 'green', 'group', 'guard', 'guess', 'guest', 'guide', 'habit', 'happy', 'heart', 'heavy', 'hello', 'honey', 'honor', 'horse', 'hotel', 'house', 'human', 'humor', 'ideal', 'image', 'imply', 'index', 'inner', 'Iraqi', 'Irish', 'issue', 'joint', 'judge', 'juice', 'knife', 'knock', 'label', 'labor', 'large', 'later', 'Latin', 'laugh', 'layer', 'learn', 'least', 'leave', 'legal', 'lemon', 'level', 'light', 'limit', 'local', 'loose', 'lover', 'lower', 'lucky', 'lunch', 'major', 'maker', 'marry', 'match', 'maybe', 'mayor', 'media', 'metal', 'meter', 'might', 'minor', 'model', 'money', 'month', 'moral', 'motor', 'mount', 'mouse', 'mouth', 'movie', 'music', 'naked', 'nerve', 'never', 'newly', 'night', 'noise', 'north', 'novel', 'nurse', 'occur', 'ocean', 'offer', 'often', 'onion', 'order', 'other', 'ought', 'owner', 'paint', 'panel', 'paper', 'party', 'patch', 'pause', 'peace', 'phase', 'phone', 'photo', 'piano', 'piece', 'pilot', 'pitch', 'place', 'plane', 'plant', 'plate', 'point', 'porch', 'pound', 'power', 'press', 'price', 'pride', 'prime', 'print', 'prior', 'proof', 'proud', 'prove', 'quick', 'quiet', 'quite', 'quote', 'radio', 'raise', 'range', 'rapid', 'ratio', 'reach', 'react', 'ready', 'refer', 'relax', 'reply', 'rifle', 'right', 'river', 'rough', 'round', 'route', 'rural', 'salad', 'sales', 'sauce', 'scale', 'scene', 'scope', 'score', 'seize', 'sense', 'serve', 'seven', 'shade', 'shake', 'shall', 'shape', 'share', 'sharp', 'sheet', 'shelf', 'shell', 'shift', 'shine', 'shirt', 'shock', 'shoot', 'shore', 'short', 'shout', 'shrug', 'sight', 'since', 'skill', 'slave', 'sleep', 'slice', 'slide', 'small', 'smart', 'smell', 'smile', 'smoke', 'solar', 'solid', 'solve', 'sorry', 'sound', 'south', 'space', 'speak', 'speed', 'spend', 'split', 'sport', 'staff', 'stage', 'stair', 'stake', 'stand', 'stare', 'start', 'state', 'steal', 'steel', 'stick', 'still', 'stock', 'stone', 'store', 'storm', 'story', 'strip', 'study', 'stuff', 'style', 'sugar', 'super', 'swear', 'sweep', 'sweet', 'swing', 'table', 'taste', 'teach', 'terms', 'thank', 'their', 'theme', 'there', 'these', 'thick', 'thing', 'think', 'third', 'those', 'three', 'throw', 'tight', 'tired', 'title', 'today', 'tooth', 'topic', 'total', 'touch', 'tough', 'tower', 'trace', 'track', 'trade', 'trail', 'train', 'treat', 'trend', 'trial', 'tribe', 'trick', 'troop', 'truck', 'truly', 'trust', 'truth', 'twice', 'uncle', 'under', 'union', 'until', 'upper', 'urban', 'usual', 'value', 'video', 'virus', 'visit', 'vital', 'voice', 'voter', 'waste', 'watch', 'water', 'weigh', 'wheel', 'where', 'which', 'while', 'white', 'whole', 'whose', 'woman', 'works', 'world', 'worry', 'worth', 'would', 'wound', 'write', 'wrong', 'yield', 'young', 'yours', 'youth', 'abuse', 'adult', 'allah', 'angie', 'angry', 'arabs', 'argie', 'asian', 'asses', 'balls', 'beast', 'bible', 'bitch', 'black', 'blind', 'boang', 'bogan', 'bombs', 'boner', 'boobs', 'booby', 'boody', 'boong', 'booty', 'bunga', 'chink', 'choad', 'chode', 'cocky', 'cohee', 'color', 'cooly', 'cra5h', 'crabs', 'crack', 'crash', 'crime', 'darky', 'death', 'demon', 'devil', 'dildo', 'dirty', 'drunk', 'eatme', 'enema', 'enemy', 'erect', 'fagot', 'fairy', 'faith', 'fatah', 'fatso', 'feces', 'felch', 'fight', 'forni', 'fraud', 'fubar', 'fucck', 'fucka', 'fucks', 'fugly', 'fuuck', 'ginzo', 'girls', 'goyim', 'gross', 'gubba', 'gyppo', 'gyppy', 'hamas', 'harem', 'honky', 'horny', 'hoser', 'husky', 'hussy', 'hymen', 'hymie', 'idiot', 'jebus', 'jesus', 'jigga', 'jiggy', 'jihad', 'jizim', 'jizm', 'joint', 'kafir', 'kills', 'kinky', 'knife', 'kotex', 'kraut', 'latin', 'lesbo', \n",
        "'lezbe', 'lezbo', 'lezzo', 'limey', 'loser', 'lugan', 'lynch', 'mafia', 'mgger', 'mggor', 'mocky', 'moles', 'moron', 'naked', 'nasty', 'necro', 'negro', 'niger', 'nigga', 'nigra', 'nigre', 'nymph', 'osama', 'pansy', 'panti', \n",
        "'pendy', 'penis', 'piker', 'pikey', 'pixie', 'pocha', 'pocho', 'pommy', 'porno', 'prick', 'pubic', 'pussy', 'queef', 'queer', 'rabbi', 'randy', 'raped', 'raper', 'roach', 'sadis', 'sadom', 'sandm', 'satan', 'screw', 'semen', \n",
        "'seppo', 'sexed', 'shhit', 'shite', 'shits', 'shoot', 'sissy', 'skank', 'slant', 'slave', 'slime', 'slopy', 'sluts', 'slutt', 'smack', 'sodom', 'sooty', 'spank', 'sperm', 'spick', 'spunk', 'squaw', 'stagg', 'taboo', 'teste', \n",
        "'titty', 'tramp', 'trots', 'twink', 'urine', 'usama', 'vomit', 'vulva', 'whash', 'whore', 'willy', 'abroad', 'absorb', 'accept', 'access', 'accuse', 'across', 'action', 'active', 'actual', 'adjust', 'admire', 'advice', 'advise', 'affair', 'affect', 'afford', 'afraid', 'agency', 'agenda', 'almost', 'always', 'amount', 'animal', 'annual', 'answer', 'anyone', 'anyway', 'appeal', 'appear', 'around', 'arrest', 'arrive', 'artist', 'asleep', 'aspect', 'assert', 'assess', 'assign', 'assist', 'assume', 'assure', 'attach', 'attack', 'attend', 'author', 'barely', 'barrel', 'basket', 'battle', 'beauty', 'become', 'before', 'behind', 'belief', 'belong', 'beside', 'better', 'beyond', 'border', 'borrow', 'bother', 'bottle', 'bottom', 'branch', 'breast', 'breath', 'bridge', 'bright', 'broken', 'budget', 'bullet', 'burden', 'butter', 'button', 'camera', 'campus', 'cancer', 'carbon', 'career', 'center', 'chance', 'change', 'charge', 'cheese', 'choice', 'choose', 'church', 'circle', 'client', 'clinic', 'closer', 'coffee', 'column', 'comedy', 'commit', 'common', 'cookie', 'corner', 'cotton', 'county', 'couple', 'course', 'cousin', 'create', 'credit', 'crisis', 'critic', 'custom', 'damage', 'danger', 'dealer', 'debate', 'decade', 'decide', 'deeply', 'defeat', 'defend', 'define', 'degree', 'demand', 'depend', 'depict', 'deputy', 'derive', 'desert', 'design', 'desire', 'detail', 'detect', 'device', 'devote', 'differ', 'dining', 'dinner', 'direct', 'divide', 'doctor', 'double', 'driver', 'during', 'easily', 'editor', 'effect', 'effort', 'either', 'e-mail', 'emerge', 'employ', 'enable', 'energy', 'engage', 'engine', 'enough', 'ensure', 'entire', 'escape', 'estate', 'ethics', 'ethnic', 'evolve', 'exceed', 'except', 'expand', 'expect', 'expert', 'expose', 'extend', 'extent', 'fabric', 'factor', 'fairly', 'family', 'famous', 'farmer', 'father', 'fellow', 'female', 'figure', 'finger', 'finish', 'flavor', 'flight', 'flower', 'follow', 'forest', 'forget', 'formal', 'former', 'fourth', 'freeze', 'French', 'friend', 'future', 'galaxy', 'garage', 'garden', 'garlic', 'gather', 'gender', 'gently', 'German', 'gifted', 'glance', 'global', 'golden', 'ground', 'growth', 'guilty', 'handle', 'happen', 'hardly', 'health', 'heaven', 'height', 'highly', 'honest', 'horror', 'hungry', 'hunter', 'ignore', 'impact', 'impose', 'income', 'indeed', 'Indian', 'infant', 'inform', 'injury', 'inside', 'insist', 'intend', 'invest', 'invite', 'island', 'itself', 'jacket', 'Jewish', 'junior', 'killer', 'latter', 'launch', 'lawyer', 'leader', 'league', 'legacy', 'legend', 'length', 'lesson', 'letter', 'likely', 'listen', 'little', 'living', 'locate', 'lovely', 'mainly', 'makeup', 'manage', 'manner', 'margin', 'market', 'master', 'matter', 'medium', 'member', 'memory', 'mental', 'merely', 'method', 'middle', 'minute', 'mirror', 'mm-hmm', 'modern', 'modest', 'moment', 'mostly', 'mother', 'motion', 'murder', 'muscle', 'museum', 'Muslim', 'mutual', 'myself', 'narrow', 'nation', 'native', 'nature', 'nearby', 'nearly', 'nobody', 'normal', 'notice', 'notion', 'number', 'object', 'obtain', 'occupy', 'office', 'online', 'oppose', 'option', 'orange', 'origin', 'others', 'parent', 'partly', 'people', 'pepper', 'period', 'permit', 'person', 'phrase', 'planet', 'player', 'please', 'plenty', 'pocket', 'poetry', 'police', 'policy', 'potato', 'powder', 'prayer', 'prefer', 'pretty', 'priest', 'prison', 'profit', 'prompt', 'proper', 'public', 'pursue', 'racial', 'rarely', 'rather', 'rating', 'reader', 'really', 'reason', 'recall', 'recent', 'recipe', 'record', 'reduce', 'reform', 'refuse', 'regard', 'regime', 'region', 'reject', 'relate', 'relief', 'remain', 'remind', 'remote', 'remove', 'repeat', 'report', 'resist', 'resort', 'result', 'retain', 'retire', 'return', 'reveal', 'review', 'rhythm', 'sacred', 'safety', 'salary', 'sample', 'saving', 'scared', 'scheme', 'school', 'scream', 'screen', 'script', 'search', 'season', 'second', 'secret', 'sector', 'secure', 'select', 'Senate', 'senior', 'series', 'settle', 'severe', 'sexual', 'shadow', 'should', 'shower', 'signal', 'silent', 'silver', 'simple', 'simply', 'singer', 'single', 'sister', 'slight', 'slowly', 'smooth', 'soccer', 'social', 'source', 'Soviet', 'speech', 'spirit', 'spread', 'spring', 'square', 'stable', 'status', 'steady', 'stream', 'street', 'stress', 'strike', 'string', 'stroke', 'strong', 'studio', 'stupid', 'submit', 'sudden', 'suffer', 'summer', 'summit', 'supply', 'surely', 'survey', 'switch', 'symbol', 'system', 'tactic', 'talent', 'target', 'tennis', 'terror', 'thanks', 'theory', 'thirty', 'though', 'threat', 'throat', 'ticket', 'tissue', 'tomato', 'tongue', 'toward', 'travel', 'treaty', 'tunnel', 'twelve', 'twenty', 'unable', 'unique', 'United', 'unless', 'unlike', 'useful', 'valley', 'versus', 'vessel', 'victim', 'viewer', 'virtue', 'vision', 'visual', 'volume', 'wander', 'wealth', 'weapon', 'weekly', 'weight', 'widely', 'window', 'winner', 'winter', 'wisdom', 'within', 'wonder', 'wooden', 'worker', 'writer', 'yellow', 'addict', 'africa', 'areola', 'asshat', 'assman', 'attack', 'babies', 'beaner', 'beaver', 'biatch', 'bigass', 'bigger', 'bitchy', 'biteme', 'blacks', 'bohunk', 'boonga', 'boonie', 'breast', 'bugger', 'buried', 'byatch', 'cacker', 'cancer', 'chinky', 'christ', 'church', 'coitus', 'commie', 'condom', 'coolie', 'crappy', 'creamy', 'crimes', 'crotch', 'cummer', 'cunntt', 'dahmer', 'dammit', 'damnit', 'darkie', 'desire', 'diddle', 'doodoo', 'doodoo', 'dyefly', 'escort', 'ethnic', 'faeces', 'faggot', 'failed', 'farty', 'fatass', 'fckcum', 'feltch', 'fetish', 'firing', 'fister', 'flange', 'flydie', 'flydye', 'fondle', 'fucked', 'fucker', 'fuckin', 'fuckit', 'fungus', 'geezer', 'german', 'gringo', 'gummer', 'gyppie', 'harder', 'hardon', 'heroin', 'herpes', 'hijack', 'hindoo', 'hitler', 'hodgie', 'honger', 'honkey', 'hooker', 'horney', 'hummer', 'iblowu', 'incest', 'insest', 'israel', 'jewish', 'jizzim', 'jizzum', 'kaffer', 'kaffir', 'kaffre', 'kanake', 'kigger', 'killed', 'killer', 'kondum', 'krappy', 'kummer', 'lesbin', 'libido', 'licker', 'lickme', 'liquor', 'lolita', 'looser', 'lotion', 'macaca', 'mockey', 'mockie', 'molest', 'mormon', 'moslem', 'murder', 'muslim', 'negros', 'niggah', 'niggaz', 'nigger', 'niggle', 'niggor', 'niggur', 'niglet', 'nignog', 'nipple', 'nittit', 'nlgger', 'nlggor', 'nookey', 'nookie', 'noonan', 'nooner', 'nudger', 'orgasm', 'orgies', 'pecker', 'penile', 'period', 'phuked', 'pimped', 'pimper', 'pissed', 'pisser', 'pistol', 'polack', 'pommie', 'pooper', 'popimp', 'pudboy', 'pussie', 'racial', 'racist', 'rapist', 'rectum', 'reefer', 'reject', 'retard', 'ribbed', 'rigger', 'rimjob', 'robber', 'russki', 'sexing', 'sexpot', 'sextoy', 'sexual', 'shited', 'skanky', 'slopey', 'slutty', 'snatch', 'sniper', 'sodomy', 'soviet', 'spooge', 'spunky', 'stiffy', 'stroke', 'stupid', 'sucker', 'suckme', 'swalow', 'tampon', 'tantra', 'terror', 'tinkle', 'titjob', 'tittie', 'toilet', 'tongue', 'tortur', 'tosser', 'tranny', 'trojan', 'turnon', 'uterus', 'vagina', 'virgin', 'wanker', 'weapon', 'weenie', 'weewee', 'whites', 'whitey', 'wigger', 'willie', 'womens', 'wuzzie', 'yankee', 'zigabo', 'abandon', 'ability', 'absence', 'account', 'achieve', 'acquire', 'actress', 'address', 'advance', 'adviser', 'African', 'against', 'airline', 'airport', 'alcohol', 'already', 'amazing', 'analyst', 'analyze', 'ancient', 'another', 'anxiety', 'anybody', 'anymore', 'appoint', 'approve', 'arrange', 'arrival', 'article', 'assault', 'athlete', 'attempt', 'attract', 'average', 'balance', 'barrier', 'battery', 'because', 'bedroom', 'believe', 'beneath', 'benefit', 'besides', 'between', 'billion', 'blanket', 'bombing', 'breathe', 'briefly', 'British', 'brother', 'cabinet', 'capable', 'capital', 'captain', 'capture', 'careful', 'carrier', 'ceiling', 'central', 'century', 'certain', 'chamber', 'channel', 'chapter', 'charity', 'chicken', \n",
        "'Chinese', 'citizen', 'classic', 'clearly', 'climate', 'closely', 'clothes', 'cluster', 'collect', 'college', 'combine', 'comfort', 'command', 'comment', 'company', 'compare', 'compete', 'complex', 'compose', 'concept', 'concern', 'concert', 'conduct', 'confirm', 'connect', 'consist', 'consume', 'contact', 'contain', 'content', 'contest', 'context', 'control', 'convert', 'cooking', 'correct', 'council', 'counter', 'country', 'courage', 'crucial', 'culture', 'curious', 'current', 'declare', 'decline', 'defense', 'deficit', 'deliver', 'deserve', 'despite', 'destroy', 'develop', 'digital', 'discuss', 'disease', 'dismiss', 'display', 'dispute', 'distant', 'diverse', 'divorce', 'drawing', 'eastern', 'economy', 'edition', 'educate', 'elderly', 'element', 'embrace', 'emotion', 'English', 'enhance', 'episode', 'equally', 'evening', 'exactly', 'examine', 'example', 'exhibit', 'expense', 'explain', 'explode', 'explore', 'express', 'extreme', 'factory', 'faculty', 'failure', 'fantasy', 'fashion', 'feature', 'federal', 'feeling', 'fiction', 'fifteen', 'fighter', 'finally', 'finance', 'finding', 'fishing', 'fitness', 'foreign', 'forever', 'formula', 'fortune', 'forward', 'founder', 'freedom', 'funding', 'funeral', 'gallery', 'general', 'genetic', 'gesture', 'grocery', 'growing', 'habitat', 'handful', 'healthy', 'hearing', 'heavily', 'helpful', 'herself', 'highway', 'himself', 'history', 'holiday', 'horizon', 'housing', 'however', 'hundred', 'hunting', 'husband', 'illegal', 'illness', 'imagine', 'impress', 'improve', 'include', 'initial', 'inquiry', 'insight', 'inspire', 'install', 'instead', 'intense', 'involve', 'Islamic', 'Israeli', 'Italian', 'journal', 'journey', 'justice', 'justify', 'killing', 'kitchen', 'largely', 'lawsuit', 'leading', 'leather', 'liberal', 'library', 'license', 'limited', 'machine', 'manager', 'married', 'massive', 'meaning', 'measure', 'medical', 'meeting', 'mention', 'message', 'Mexican', 'million', 'miracle', 'missile', 'mission', 'mistake', 'mixture', 'monitor', 'morning', 'musical', 'mystery', 'natural', 'neither', 'nervous', 'network', 'nothing', 'nowhere', 'nuclear', 'observe', 'obvious', 'offense', 'officer', 'Olympic', 'ongoing', 'opening', 'operate', 'opinion', 'organic', 'outcome', 'outside', 'overall', 'package', 'painful', 'painter', 'parking', 'partner', 'passage', 'passion', 'patient', 'pattern', 'payment', 'penalty', 'perfect', 'perform', 'perhaps', 'picture', 'plastic', 'popular', 'portion', 'portray', \n",
        "'possess', 'poverty', 'predict', 'prepare', 'present', 'pretend', 'prevent', 'primary', 'privacy', 'private', 'problem', 'proceed', 'process', 'produce', 'product', 'profile', 'program', 'project', 'promise', 'promote', 'propose', 'protect', 'protein', 'protest', 'provide', 'publish', 'purpose', 'qualify', 'quality', 'quarter', 'quickly', 'quietly', 'radical', 'rapidly', 'reading', 'reality', 'realize', 'receive', 'recover', 'recruit', 'reflect', 'refugee', 'regular', 'release', 'replace', 'request', 'require', 'resolve', 'respect', 'respond', 'restore', 'revenue', 'roughly', 'routine', 'running', 'Russian', 'satisfy', 'scandal', 'scholar', 'science', 'section', 'segment', 'senator', 'serious', 'service', 'session', 'setting', 'several', 'shelter', 'shortly', 'silence', 'similar', 'society', 'soldier', 'somehow', 'someone', 'Spanish', 'speaker', 'special', 'species', 'squeeze', 'station', 'stomach', 'storage', 'strange', 'stretch', 'student', 'subject', 'succeed', 'success', 'suggest', 'suicide', 'support', 'suppose', 'Supreme', 'surface', 'surgery', 'survive', 'suspect', 'sustain', 'symptom', 'teacher', 'tension', 'testify', 'testing', 'theater', 'therapy', 'thought', 'through', 'tobacco', 'tonight', 'totally', 'tourist', 'towards', 'traffic', 'tragedy', 'trouble', 'typical', 'undergo', 'uniform', 'unknown', 'unusual', 'usually', 'utility', 'variety', 'various', 'vehicle', 'venture', 'version', 'veteran', 'victory', 'village', 'violate', 'violent', 'visible', 'visitor', 'warning', 'wealthy', 'weather', 'wedding', 'weekend', 'welcome', 'welfare', 'western', 'whereas', 'whether', 'whisper', 'willing', 'without', 'witness', 'working', 'worried', 'writing', 'addicts', 'african', 'amateur', 'analsex', 'aroused', 'assault', 'assfuck', 'asshole', 'asshore', 'asskiss', 'asslick', 'asswipe', 'badfuck', 'banging', 'baptist', 'barface', 'bazooms', 'beatoff', 'bestial', 'bigbutt', 'bitcher', 'bitches', 'bitchez', 'bitchin', 'blowjob', 'bollick', 'bollock', 'bombers', 'bombing', 'bondage', 'boobies', 'brothel', 'buggery', 'bumfuck', 'buttman', 'carruth', 'chinese', 'clogwog', 'cocaine', 'cocknob', 'colored', 'coondog', 'crapola', 'crapper', 'cumfest', 'cumming', 'cumquat', 'cumshot', 'deposit', 'destroy', 'dickman', 'dickwad', 'dipshit', 'disease', 'drunken', 'dumbass', 'ecstacy', 'execute', 'fagging', 'failure', 'fairies', 'farted', 'fatfuck', 'felcher', 'fisting', 'flasher', 'fuckbag', 'fuckers', 'fuckher', 'fuckina', 'fucking', 'fuckme', 'fuckoff', 'fuckpig', 'fuckyou', 'funeral', 'funfuck', 'gangsta', 'gaysex', 'genital', 'getiton', 'goddamn', 'handjob', 'hiscock', 'honkers', 'hookers', 'hooters', 'hosejob', 'hostage', 'hotdamn', 'hustler', 'illegal', 'israeli', 'israels', 'jackass', 'jackoff', 'japcrap', 'jerkoff', 'jigaboo', 'jiggabo', 'jigger', 'jimfish', 'juggalo', 'killing', 'kissass', 'kumming', 'kumquat', 'lactate', 'lesbain', 'lesbayn', 'lesbian', 'liberal', 'livesex', 'lovegoo', 'lovegun', 'lowlife', 'lubejob', 'lucifer', 'mexican', 'mideast', 'mulatto', 'muncher', 'nastyho', 'negroes', 'negroid', 'niggard', 'niggers', 'niggers', 'niggled', 'niggles', 'pansies', 'panties', 'peehole', 'pee-pee', 'penises', 'phuking', 'phukked', 'phungky', 'pindick', 'pisses', 'pissin', 'pissing', 'playboy', 'pooping', 'poverty', 'puddboy', 'puntang', 'pussies', 'quashie', 'quickie', 'radical', 'raghead', 'rearend', 'redneck', 'reestie', 'refugee', 'remains', 'rimming', 'russkie', 'schlong', 'scrotum', 'servant', 'sexfarm', 'sextogo', 'sextoys', 'shaggin', 'sheeney', 'shinola', 'shitcan', 'shitfit', 'shiting', 'shitola', 'shitted', 'shitter', 'shitty', 'skumbag', 'slapper', 'snigger', 'spitter', 'strapon', 'suckoff', 'suicide', 'swallow', 'tarbaby', 'titfuck', 'titties', 'torture', 'trannie', 'triplex', 'twinkie', 'upskirt', 'urinary', 'urinate', 'vaginal', 'vatican', 'wanking', 'waysted', 'welcher', 'welfare', 'wetback', 'wetspot', 'whacker', 'whigger', 'whiskey', 'abortion', 'absolute', 'academic', 'accident', 'accurate', 'activist', 'activity', 'actually', 'addition', 'adequate', 'advanced', 'advocate', 'aircraft', 'alliance', 'although', 'American', 'analysis', 'announce', 'anything', 'anywhere', 'apparent', 'approach', 'approval', 'argument', 'artistic', 'athletic', 'attitude', 'attorney', 'audience', 'baseball', 'bathroom', 'behavior', 'birthday', 'boundary', 'building', 'business', 'campaign', 'Canadian', 'capacity', 'category', 'Catholic', 'ceremony', 'chairman', 'champion', 'changing', 'chemical', 'civilian', 'clinical', 'clothing', 'collapse', 'colonial', 'complain', 'complete', 'computer', 'conclude', 'concrete', 'conflict', 'confront', 'Congress', 'consider', 'constant', 'consumer', 'continue', 'contract', 'contrast', 'convince', 'coverage', 'creation', 'creative', 'creature', 'criminal', 'criteria', 'critical', 'cultural', 'customer', 'darkness', 'daughter', 'decision', 'decrease', 'delivery', 'Democrat', 'describe', 'designer', 'detailed', 'dialogue', 'directly', 'director', 'disagree', 'disaster', 'discover', 'disorder', 'distance', 'distinct', 'district', 'division', 'document', 'domestic', 'dominant', 'dominate', 'downtown', 'dramatic', 'earnings', 'economic', 'educator', 'election', 'electric', 'emission', 'emphasis', 'employee', 'employer', 'engineer', 'enormous', 'entirely', 'entrance', 'estimate', 'European', 'evaluate', 'everyday', 'everyone', 'evidence', 'exchange', 'exciting', 'exercise', 'existing', 'exposure', 'external', 'facility', 'familiar', 'favorite', 'fighting', 'football', 'frequent', 'friendly', 'function', 'generate', 'governor', 'graduate', 'greatest', 'headline', 'heritage', 'historic', 'homeless', 'hospital', 'identify', 'identity', 'incident', 'increase', 'indicate', 'industry', 'innocent', 'instance', 'interest', 'internal', 'Internet', 'invasion', 'investor', 'involved', 'Japanese', 'judgment', 'language', 'learning', 'lifetime', 'literary', 'location', 'magazine', 'maintain', 'majority', 'marriage', 'material', 'medicine', 'military', 'minister', 'minority', 'moderate', 'moreover', 'mortgage', 'mountain', 'movement', 'multiple', 'musician', 'national', 'negative', 'neighbor', 'normally', 'northern', 'numerous', 'observer', 'occasion', 'official', 'operator', 'opponent', 'opposite', 'ordinary', 'organize', 'original', 'overcome', 'overlook', 'painting', 'perceive', 'personal', 'persuade', 'physical', 'planning', 'platform', 'pleasure', 'politics', 'portrait', 'position', 'positive', 'possible', 'possibly', 'powerful', 'practice', 'pregnant', 'presence', 'preserve', 'pressure', 'previous', 'priority', 'prisoner', 'probably', 'producer', 'progress', 'properly', 'property', 'proposal', 'proposed', 'prospect', 'provider', 'province', 'publicly', 'purchase', 'question', 'reaction', 'recently', 'recovery', 'regional', 'register', 'regulate', 'relation', 'relative', 'relevant', 'religion', 'remember', 'reporter', 'research', 'resemble', 'resident', 'resource', 'response', 'romantic', 'sanction', 'scenario', 'schedule', 'security', 'sentence', 'separate', 'sequence', 'shooting', 'shopping', 'shoulder', 'slightly', 'software', 'solution', 'somebody', 'somewhat', 'southern', 'specific', 'spending', 'standard', 'standing', 'straight', 'stranger', 'strategy', 'strength', 'strongly', 'struggle', 'suddenly', 'supposed', 'surprise', 'surround', 'survival', 'survivor', 'taxpayer', 'teaching', 'teaspoon', 'teenager', 'tendency', 'terrible', 'thinking', 'thousand', 'threaten', 'together', 'tomorrow', 'training', 'transfer', 'ultimate', 'universe', 'unlikely', 'vacation', 'valuable', 'variable', 'violence', 'whatever', 'whenever', 'withdraw', 'workshop', 'yourself', 'abortion', 'american', 'arsehole', 'assassin', 'assclown', 'assholes', 'assklown', 'asslover', 'assmunch', 'asswhore', 'backdoor', 'backseat', 'ballsack', 'barfface', 'bastard', 'bazongas', 'beastial', 'beat-off', 'bisexual', 'bitching', 'blackman', 'blackout', 'buggered', 'bullcrap', 'bulldike', 'bulldyke', 'bullshit', 'bunghole', 'buttbang', 'buttface', 'buttfuck', 'butthead', 'buttplug', 'cameltoe', 'canadian', 'catholic', 'cemetery', 'chinaman', 'chinamen', 'clitoris', 'cockhead', 'cockknob', 'cocksman', 'cocktail', 'coloured', 'copulate', 'cornhole', 'criminal', 'cumqueen', 'cunteyed', 'cuntfuck', 'cybersex', 'datnigga', 'defecate', 'dickhead', 'dickless', 'dicklick', 'dickweed', 'dipstick', 'diseases', 'dripdick', 'dumbfuck', 'easyslut', 'eatballs', 'eatpussy', 'erection', 'european', 'executed', 'farting', 'fastfuck', 'felatio', 'felching', 'fellatio', 'feltcher', 'filipina', 'filipino', 'fistfuck', 'footfuck', 'footstar', 'foreskin', 'foursome', 'freefuck', 'fuckable', 'fuckedup', 'fuckface', 'fuckfest', 'fuckhead', 'fuckknob', 'fucktard', 'gangbang', 'givehead', 'godammit', 'goddamit', 'gonzagas', 'gotohell', 'headfuck', 'henhouse', 'hijacker', 'homicide', 'horniest', 'hotpussy', 'intheass', 'italiano', 'jackshit', 'japanese', 'jijjiboo', 'jizjuice', 'knockers', 'lapdance', 'limpdick', 'lingerie', 'lovebone', 'manhater', 'manpaste', 'meatrack', 'minority', 'molester', 'molestor', 'mosshead', 'muffdive', 'murderer', 'narcotic', 'nigerian', 'niggards', 'niggards', 'niggling', 'ontherag', 'orgasim', 'peepshow', 'peepshpw', 'phonesex', 'phukking', 'pimpjuic', 'pimpsimp', 'pisshead', 'pissoff', 'playgirl', 'poontang', 'pornking', 'pussycat', 'radicals', 'redlight', 'retarded', 'roundeye', 'screwyou', 'sexhound', 'sexhouse', 'sexslave', 'sexually', 'sexwhore', 'sexymoma', 'shagging', 'shitdick', 'shitface', 'shitfuck', 'shitfull', 'shithead', 'shitlist', 'shitting', 'shooting', 'showtime', 'slanteye', 'slutting', 'slutwear', 'sniggers', 'snowback', 'sodomise', 'sodomite', 'sodomize', 'spermbag', 'spigotty', 'stringer', \n",
        "'stroking', 'suckdick', 'swastika', 'syphilis', 'testicle', 'thirdeye', 'thirdleg', 'threeway', 'titlover', 'tuckahoe', 'uptheass', 'vibrater', 'vibrator', 'vietcong', 'violence', 'accompany', 'according', 'admission', 'advantage', 'adventure', 'afternoon', 'agreement', 'apartment', 'architect', 'assistant', 'associate', 'attention', 'attribute', 'authority', 'available', 'awareness', 'basically', 'beautiful', 'beginning', 'boyfriend', 'breakfast', 'brilliant', 'calculate', 'candidate', 'carefully', 'celebrate', 'celebrity', 'certainly', 'challenge', 'character', 'childhood', 'chocolate', 'Christian', 'Christmas', 'cigarette', 'classroom', 'coalition', 'cognitive', 'colleague', 'commander', 'committee', 'community', 'complaint', 'component', 'concerned', 'condition', 'confident', 'confusion', 'consensus', 'construct', 'container', 'continued', 'corporate', 'counselor', 'criticism', 'criticize', 'currently', 'dangerous', 'defendant', 'defensive', 'democracy', 'dependent', 'depending', 'desperate', 'determine', 'different', 'difficult', 'dimension', 'direction', 'disappear', 'discourse', 'discovery', 'diversity', 'economics', 'economist', 'education', 'effective', 'efficient', 'eliminate', 'elsewhere', 'emergency', 'emotional', 'emphasize', 'encounter', 'encourage', 'equipment', 'essential', 'establish', 'everybody', 'evolution', \n",
        "'excellent', 'exception', 'executive', 'existence', 'expansion', 'expensive', 'explosion', 'extension', 'extensive', 'extremely', 'financial', 'following', 'formation', 'framework', 'frequency', 'furniture', 'generally', 'gentleman', 'gradually', 'guarantee', 'guideline', 'highlight', 'historian', 'household', 'immediate', 'immigrant', 'implement', 'important', 'incentive', 'including', 'increased', 'infection', 'inflation', 'influence', 'initially', 'insurance', 'intensity', 'intention', 'interpret', 'interview', 'introduce', 'knowledge', 'landscape', 'lifestyle', 'literally', 'long-term', 'marketing', 'meanwhile', 'mechanism', 'narrative', 'naturally', 'necessary', 'negotiate', 'newspaper', 'objective', 'obviously', 'offensive', 'operating', 'operation', 'otherwise', 'ourselves', 'passenger', 'perfectly', 'permanent', 'personnel', 'physician', 'political', 'pollution', 'potential', 'practical', 'precisely', 'pregnancy', 'president', 'primarily', 'principal', 'principle', 'procedure', 'professor', 'prominent', 'provision', 'publisher', 'recognize', 'recommend', 'recording', 'reduction', 'reference', 'regarding', 'regularly', 'reinforce', 'religious', 'remaining', 'represent', 'satellite', 'scientist', 'secretary', 'selection', 'sensitive', 'seriously', 'similarly', 'situation', 'so-called', 'something', 'sometimes', 'somewhere', 'spiritual', 'spokesman', 'stability', 'statement', 'strategic', 'structure', 'substance', 'supporter', 'surprised', 'technical', 'technique', 'telephone', 'telescope', 'temporary', 'territory', 'terrorism', 'terrorist', 'testimony', 'therefore', 'tradition', 'transform', 'translate', 'treatment', 'typically', 'universal', 'variation', 'vegetable', 'violation', 'virtually', 'volunteer', 'wonderful', 'yesterday', 'analannie', 'assbagger', 'asscowboy', 'assfucker', 'assjockey', 'asskisser', 'asslicker', 'assmonkey', 'asspacker', 'asspirate', 'assranger', 'bicurious', 'bitchslap', 'bootycall', 'bountybar', 'breastjob', 'breastman', 'butchdike', 'butchdyke', 'butt-bang', 'buttmunch', 'buttstain', 'catholics', 'childrens', 'christian', 'cigarette', 'clamdiver', 'cockblock', 'cockfight', 'cocklover', 'cockqueen', 'cockrider', 'cocksmith', 'cocksucer', 'cocksuck', 'cocktease', 'communist', 'crackpipe', 'criminals', 'crotchrot', 'cumbubble', 'cumjockey', 'cuntlick', 'damnation', 'dickbrain', 'disturbed', 'dixiedike', 'dixiedyke', 'dragqueen', 'dragqween', 'dumbbitch', 'ejaculate', 'ethiopian', 'excrement', 'execution', 'explosion', 'fatfucker', 'feltching', 'fornicate', 'freakfuck', 'fuckbuddy', 'fuckfreak', 'fuckwhore', 'gatorbait', 'goddammit', 'goddamned', 'goddamnes', 'goddamnit', 'hijacking', 'hitlerism', 'hitlerist', 'horseshit', 'hottotrot', 'inthebuff', 'kumbubble', 'kumbullbe', 'loadedgun', 'lovejuice', 'magicwand', 'marijuana', 'mastabate', 'moneyshot', 'mothafuck', 'muffdiver', 'nastyslut', 'nigerians', 'niggarded', 'niggardly', 'nigglings', 'nutfucker', 'penthouse', 'picaninny', 'pimpjuice', 'pornflick', 'premature', 'prickhead', 'primetime', 'pubiclice', 'pussylips', 'rearentry', 'rentafuck', 'scallywag', 'sexkitten', 'sexy-slim', 'shiteater', 'shitfaced', 'shithouse', 'shitstain', 'shortfuck', 'sixsixsix', 'sixtynine', 'skankfuck', 'skinflute', 'slaughter', 'sleezebag', 'slideitin', 'slimeball', 'slopehead', 'slutwhore', 'sniggered', \"snigger's\", 'splittail', 'stripclub', 'suckmyass', 'suckmytit', 'swallower', 'sweetness', 'terrorist', 'testicles', 'thicklips', 'threesome', 'titfucker', 'titfuckin', 'titlicker', 'towelhead', 'trisexual', 'upthebutt', 'yellowman', 'zipperhead', 'absolutely', 'accomplish', 'additional', 'adjustment', 'adolescent', 'aggressive', 'anticipate', 'apparently', 'appearance', 'appreciate', 'assessment', 'assignment', 'assistance', 'assumption', 'atmosphere', 'attractive', 'background', 'basketball', 'biological', 'capability', 'collection', 'collective', 'commercial', 'commission', 'commitment', 'comparison', 'competitor', 'completely', 'conclusion', 'conference', 'confidence', 'connection', 'consistent', 'constantly', \n",
        "'constitute', 'consultant', 'contribute', 'convention', 'conviction', 'curriculum', 'definitely', 'definition', 'democratic', 'department', 'depression', 'developing', 'difference', 'difficulty', 'disability', 'discipline', 'discussion', 'distribute', 'efficiency', 'electronic', 'elementary', 'employment', 'enterprise', 'especially', 'evaluation', 'eventually', 'everything', 'everywhere', 'exhibition', 'experience', 'experiment', 'expression', 'foundation', 'frequently', 'friendship', 'generation', 'girlfriend', 'government', 'helicopter', 'historical', 'hypothesis', 'illustrate', 'importance', 'impossible', 'impression', 'impressive', 'increasing', 'incredible', 'indication', 'individual', 'industrial', 'ingredient', 'initiative', 'instructor', 'instrument', 'interested', 'investment', 'journalist', 'laboratory', 'leadership', 'legitimate', 'limitation', 'literature', 'management', 'medication', 'membership', 'motivation', 'nomination', 'obligation', 'occupation', 'opposition', 'originally', 'particular', 'percentage', 'perception', 'permission', 'personally', 'phenomenon', 'philosophy', 'photograph', 'physically', 'politician', 'population', 'preference', 'previously', 'production', 'profession', 'proportion', 'prosecutor', 'protection', 'psychology', 'punishment', 'reasonable', 'reflection', 'regardless', 'regulation', 'relatively', 'remarkable', 'repeatedly', 'Republican', 'reputation', 'researcher', 'resistance', 'resolution', 'respondent', 'restaurant', 'retirement', 'revolution', 'scientific', 'settlement', 'specialist', 'statistics', 'strengthen', 'subsequent', 'successful', 'sufficient', 'suggestion', 'surprising', 'tablespoon', 'technology', 'television', 'themselves', 'throughout', 'tournament', 'transition', 'tremendous', 'ultimately', 'understand', 'university', 'vulnerable', 'widespread', 'assblaster', 'assmuncher', 'asspuppies', 'australian', 'balllicker', 'beastality', 'bestiality', 'bigbastard', 'bumblefuck', 'butchbabes', 'buttfucker', 'buttpirate', 'chickslick', 'clamdigger', 'cockcowboy', 'cocklicker', 'cocksmoker', 'cocksucker', 'conspiracy', 'corruption', 'crackwhore', 'cunilingus', 'cuntfucker', 'cuntsucker', 'deapthroat', 'deepthroat', 'dicklicker', 'doggystyle', 'ejaculated', 'facefucker', 'fingerfood', 'flatulence', 'footaction', 'footfucker', 'footlicker', 'fourtwenty', 'fuckfriend', 'fuckinnuts', 'fuckmehard', 'fuckmonkey', 'gangbanger', 'gonorrehea', 'greaseball', 'headlights', 'homosexual', 'kunilingus', 'lovemuscle', 'lovepistol', 'loverocket', 'mastabater', 'masterbate', 'masturbate', 'mickeyfinn', 'mothafucka', 'motherfuck', 'mufflikcer', 'nastybitch', 'nastywhore', 'niggaracci', 'niggarding', 'niggerhead', 'niggerhole', 'nipplering', 'palesimian', 'peckerwood', 'piccaninny', 'pickaninny', 'pocketpool', 'propaganda', 'prostitute', 'protestant', 'pussyeater', 'pussylover', 'republican', 'samckdaddy', 'sandnigger', 'shawtypimp', 'shitfucker', 'shithapens', 'sixtyniner', 'skankbitch', 'skankwhore', 'sleezeball', 'sniggering', 'snownigger', 'sonofbitch', 'spermacide', 'stupidfuck', 'suckmydick', 'transexual', 'unfuckable', 'whiskydick', 'whitetrash', 'whorehouse', \n",
        "'achievement', 'acknowledge', 'advertising', 'alternative', 'anniversary', 'application', 'appointment', 'appropriate', 'arrangement', 'association', 'celebration', 'cholesterol', 'combination', 'comfortable', 'communicate', \n",
        "'competition', 'competitive', 'complicated', 'composition', 'concentrate', 'consequence', 'consumption', 'controversy', 'cooperation', 'corporation', 'demonstrate', 'description', 'destruction', 'development', 'differently', \n",
        "'distinction', 'distinguish', 'educational', 'effectively', 'electricity', 'enforcement', 'engineering', 'environment', 'essentially', 'examination', 'expectation', 'explanation', 'frustration', 'fundamental', 'furthermore', \n",
        "'grandfather', 'grandmother', 'imagination', 'immediately', 'immigration', 'implication', 'improvement', 'incorporate', 'independent', 'information', 'institution', 'instruction', 'interaction', 'interesting', 'investigate', \n",
        "'involvement', 'legislation', 'maintenance', 'measurement', 'necessarily', 'negotiation', 'nonetheless', 'observation', 'opportunity', 'orientation', 'Palestinian', 'participant', 'participate', 'partnership', 'performance', \n",
        "'personality', 'perspective', 'politically', 'possibility', 'potentially', 'preparation', 'publication', 'quarterback', 'recognition', 'requirement', 'reservation', 'responsible', 'restriction', 'scholarship', 'significant', \n",
        "'substantial', 'temperature', 'traditional', 'assassinate', 'backdoorman', 'barelylegal', 'beastiality', 'breastlover', 'buttfuckers', 'buttmuncher', 'cameljockey', 'cockblocker', 'cocksucked', 'cocksucking', 'crack-whore', 'cunillingus', 'cunnilingus', 'cuntlicker', 'cyberslimer', 'dingleberry', 'doggiestyle', 'ejaculation', 'executioner', 'fannyfucker', 'fingerfuck', 'fistfucked', 'fistfucker', 'fuckinright', 'fudgepacker', 'gangbanged', 'glazeddonut', 'hillbillies', 'holestuffer', 'homobangers', 'intercourse', 'interracial', 'jesuschrist', 'junglebunny', 'kunnilingus', 'mastrabator', 'meatbeatter', 'molestation', 'mooncricket', 'mothafuckaz', 'mothafucker', 'mothafuckin', 'muffindiver', 'palestinian', 'penetration', 'porchmonkey', 'pornography', 'pussyfucker', 'pussylicker', 'shithappens', 'skankybitch', 'skankywhore', 'slavedriver', 'slimebucket', 'snatchpatch', 'sonofabitch', 'spermherder', 'spreadeagle', 'tonguetramp', 'transsexual', 'twobitwhore', 'whiskeydick', 'whitenigger', 'whorefucker', 'agricultural', 'championship', 'characterize', 'circumstance', 'conservative', 'considerable', 'construction', 'contemporary', 'contribution', 'conventional', 'conversation', 'distribution', 'dramatically', 'headquarters', 'increasingly', 'independence', 'intellectual', 'intelligence', 'intervention', 'introduction', 'investigator', \n",
        "'manufacturer', 'neighborhood', 'nevertheless', 'occasionally', 'organization', 'particularly', 'photographer', 'prescription', 'presentation', 'presidential', 'professional', 'psychologist', 'relationship', 'satisfaction', 'significance', 'specifically', 'successfully', 'surprisingly', 'athletesfoot', 'beatyourmeat', 'cherrypopper', 'conservative', 'crotchjockey', 'crotchmonkey', 'cuntlicking', 'ejaculating', 'fistfucking', 'freakyfucker', 'fuckingbitch', 'goldenshower', 'grostulation', 'heterosexual', 'lezbefriends', 'masturbating', 'mothafucked', 'motherfucked', 'motherfucker', 'motherfuckin', 'nofuckingway', 'pornprincess', 'pussypounder', 'spermhearder', 'stupidfucker', 'timbernigger', 'titbitnipply', 'tonguethrust', 'trailertrash', 'transvestite', 'tunneloflove', 'williewanker', 'administrator', 'approximately', 'communication', 'comprehensive', 'concentration', 'congressional', 'consciousness', 'consideration', 'controversial', 'correspondent', 'demonstration', 'entertainment', 'environmental', 'establishment', 'extraordinary', 'institutional', 'international', 'investigation', 'manufacturing', 'participation', 'psychological', 'significantly', 'sophisticated', 'understanding', 'unfortunately', 'alligatorbait', 'assassination', 'carpetmuncher', 'dickforbrains', 'fingerfucked', 'fingerfucker', 'fingerfuckers', 'jacktheripper', 'masterblaster', 'mothafucking', 'mothafuckings', 'motherfucking', 'niggardliness', 'pearlnecklace', 'pooperscooper', 'shitforbrains', 'shitoutofluck', 'tongethruster', 'virginbreaker', 'administration', 'characteristic', 'constitutional', 'discrimination', 'identification', 'interpretation', 'recommendation', 'representation', 'representative', 'responsibility', 'transformation', 'transportation', 'fingerfucking', 'luckycammeltoe', 'motherfuckings', 'motherlovebone', 'niggardlinesss', 'poorwhitetrash', 'purinapricness', 'smackthemonkey', 'spankthemonkey', 'devilworshipper', 'spaghettibender', 'spaghettinigger', 'African-American', 'mattressprincess', 'goddamnmuthafucker', 'gaymuthafuckinwhore']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8XQiE9GKNFp",
        "colab_type": "text"
      },
      "source": [
        "## Hashtag manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFD-tf8aKRzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_hashtag(tag):\n",
        "  return re.sub(\"[^a-zA-Z]+\", '', tag).lower() \n",
        "\n",
        "# returns list of lists of words\n",
        "def split_tag(tag):\n",
        "  results = []\n",
        "  for w in words:\n",
        "    if len(w) > len(tag): \n",
        "      break\n",
        "    if tag.startswith(w):\n",
        "      suffix = tag[len(w):]\n",
        "      if suffix == '':\n",
        "        return [[w]]\n",
        "      rest = split_tag(suffix)\n",
        "      for res in rest:\n",
        "        result = [w] + res\n",
        "        if sum([len(s) for s in result]) == len(tag):\n",
        "          results.append(result)\n",
        "        else:\n",
        "          print('wrong length: ' + str(sum([len(s) for s in result])) + ' != ' + str(len(tag)))\n",
        "  return results\n",
        "\n",
        "def tag_to_string(split_tag):\n",
        "  t = \"\"\n",
        "  if len(split_tag) > 0:\n",
        "    for word in split_tag[0]:\n",
        "      t = t + \" \" + word\n",
        "  return t\n",
        "\n",
        "def replace_hashtag(tweet, tag, tag_to_str):\n",
        "  return tweet.replace(tag, tag_to_str)\n",
        "\n",
        "def find_hashtags(tweet):\n",
        "  tags = re.findall(\"#[\\w]*\", tweet)\n",
        "  return tags"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyzH2t0nyli9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5313e6e9-a1ee-4577-fd09-0bf321e4d142"
      },
      "source": [
        "print(find_hashtags(df.tweet[13239]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#Spanishrevenge', '#justice', '#HumanRights', '#FreedomOfExpression', '#Spain', '#fakedemocracy', '#cddr', '#shameonSpain', '#WakeupEurope']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiXUv93afNTj",
        "colab_type": "text"
      },
      "source": [
        "This removes hashtags from a tweet and replaces it with the words in that hashtag. Though if the exact words aren't in the vocab it won't be able to do a split and then those words will in its unsplit form be added to the tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xlqnzfaL8w1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tweet in df.tweet:\n",
        "  new_tweet = tweet\n",
        "  hashtags = find_hashtags(tweet)\n",
        "  if hashtags is not None:\n",
        "    for tag in hashtags:\n",
        "      c_tag = clean_hashtag(str(tag))\n",
        "      #print(hashtag)\n",
        "      split_hashtag = split_tag(c_tag)\n",
        "      #print(split_hashtag)\n",
        "      if len(split_hashtag) > 0:\n",
        "        new_tweet = replace_hashtag(new_tweet, tag.strip(), tag_to_string(split_hashtag))\n",
        "      else:\n",
        "        new_tweet = replace_hashtag(new_tweet, tag.strip(), c_tag)\n",
        "  df.replace(tweet, new_tweet, inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkZgK9k4lIho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1c5d5c62-71f9-4f08-8398-7f673248aa80"
      },
      "source": [
        "print(df.tweet[55])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " gun control advocates must STOP falling all over themselves to assure electorate that they too love the HORRIFIC 2A URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwSSy0Y5xbGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "67ab615a-0154-4742-fd54-76e508b9a0cd"
      },
      "source": [
        "print(df.tweet[13239])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spanishrevenge vs.  justice humanrights and  freedom of expression spain is a fakedemocracy $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ $MENTION$ cddr shameonspain wakeupeurope $MENTION$ URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrBi-YDfP0t",
        "colab_type": "text"
      },
      "source": [
        "## Saving the labels and parsed tweets of the training data\n",
        "\n",
        "Saves the training data into numpy arrays. Labels are changed into binary representation where none offensive tweets are set to 0 and offensive is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5-NchQtaf9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df.tweet.values\n",
        "labels = df.subtask_a.values\n",
        "labels = np.where(labels == \"NOT\", 0, 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODfqSpdzg15z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951e4cad-c09c-448a-864c-e152d2ac483a"
      },
      "source": [
        "print(tweets[0] + \" label \" + str(labels[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$MENTION$ She should ask a few native Americans what their take on this is. label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObJOcAbQKeys",
        "colab_type": "text"
      },
      "source": [
        "## BERT Tokenization \n",
        "\n",
        "Help for how to handle the tokenization in BERT was found here: https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ_RSvVzKiZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "65d80fa5-7d2b-4933-a041-60f4bdb05d33"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 41.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3f0f3bc7b51c28afd867c33cd10095efd71c28ed9584d44d32b9fcca299f28a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6od_vKjMjpKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "06fbcd17ff8e4d6282d9721a721a2bec",
            "7984dafa37bd4fc3b57e5513ac94292b",
            "2c2e6c030fd04bb3b379326facdd4feb",
            "d98baecd08c84d9f9124f1d3facd7a03",
            "feee6e1d7bf54635b6e6e37db2f5a9c8",
            "78209bb27d0a4207b7d900ab1972a647",
            "1ae9200bdd524c04934c971ad420c912",
            "5db589b707ab4dfc9cf5b01926980529"
          ]
        },
        "outputId": "bb2b7a9e-b65c-4964-b8f4-f96b4c5080a0"
      },
      "source": [
        "# Tokenize with BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139902625113480 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpwnv8m58m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06fbcd17ff8e4d6282d9721a721a2bec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:filelock:Lock 139902625113480 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1XPmFII2Ksr",
        "colab_type": "text"
      },
      "source": [
        "## Final text clean-up and addition of special BERT tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wn8-zVDjut8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cc9bdb18-f761-4206-8838-5aee5eb4d3a4"
      },
      "source": [
        "def clean_punctutation(tweets):\n",
        "  cleaned_tweets = []\n",
        "  for tweet in tweets:\n",
        "    i = 0\n",
        "    while(i < len(tweet)-1):\n",
        "      if(isMultiplePunctuation(tweet[i], tweet[i+1])):\n",
        "        tweet= tweet[:i] + tweet[i+1:]\n",
        "      else:\n",
        "        i+=1\n",
        "    cleaned_tweets.append(tweet)\n",
        "  return cleaned_tweets\n",
        "\n",
        "def isMultiplePunctuation(c1, c2):\n",
        "  if((c1 == '.' or c1 == '?' or c1 == '!') and (c2 == '.' or c2 == '?' or c2 == '!')):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def add_sentence_seperators(tweets):\n",
        "  seperated_tweets = []\n",
        "  for tweet in tweets:\n",
        "    seperated_tweet = \"[CLS] \" + seperate_sentences(str(tweet))\n",
        "    if(seperated_tweet[-5:] != \"[SEP]\"):\n",
        "      seperated_tweet += \" [SEP]\"\n",
        "    seperated_tweets.append(seperated_tweet)\n",
        "  return seperated_tweets\n",
        "\n",
        "def seperate_sentences(tweet):\n",
        "  tweet = tweet.replace('.', \". [SEP]\")\n",
        "  tweet = tweet.replace('!', \"! [SEP]\")\n",
        "  tweet = tweet.replace('?', \"? [SEP]\")\n",
        "  return tweet\n",
        "\n",
        "cleaned_tweets = clean_punctutation(tweets)\n",
        "print(cleaned_tweets[1])\n",
        "seperated_tweets = add_sentence_seperators(cleaned_tweets)\n",
        "print(seperated_tweets[1])\n",
        "\n",
        "#Turn it to numpy array so labels and tweets are the same type\n",
        "seperated_tweets = np.array(seperated_tweets)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$MENTION$ $MENTION$ Go home youre drunk! $MENTION$ maga trump $EMOJI$$EMOJI$ URL\n",
            "[CLS] $MENTION$ $MENTION$ Go home youre drunk! [SEP] $MENTION$ maga trump $EMOJI$$EMOJI$ URL [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhW6-SIU10kW",
        "colab_type": "text"
      },
      "source": [
        "Guide on how to tokenize and turn the lists into tensors was found here: https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg7TT0nEjh_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "0119c88f-2d92-443f-f1c9-ca0d5b697b3e"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "token_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for tweet in seperated_tweets:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Map tokens to their IDs.\n",
        "    #   (3) Pad or truncate the sentence to `max_length`\n",
        "    #   (4) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        tweet,                      # tweet to encode.\n",
        "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'?\n",
        "                        max_length = 300,           # Pad & truncate all tweets.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    #Add the token seperators for each sentence\n",
        "    token_ids.append(encoded_dict['token_type_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "token_ids = torch.cat(token_ids, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print tweet 0, now as a list of IDs.\n",
        "print('Original: ', seperated_tweets[1])\n",
        "print('Input IDs:', input_ids[1])\n",
        "print('Token IDs:', token_ids[1])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  [CLS] $MENTION$ $MENTION$ Go home youre drunk! [SEP] $MENTION$ maga trump $EMOJI$$EMOJI$ URL [SEP]\n",
            "Input IDs: tensor([  101,  1002,  5254,  1002,  1002,  5254,  1002,  2175,  2188,  2115,\n",
            "         2063,  7144,   999,   102,  1002,  5254,  1002, 23848,  2050,  8398,\n",
            "         1002,  7861, 29147,  2072,  1002,  1002,  7861, 29147,  2072,  1002,\n",
            "        24471,  2140,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Token IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAkYkJjHbN4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "c84c20d6-5779-4766-b5b2-534b3c57ad5e"
      },
      "source": [
        "# Print tweet 0, now as a list of IDs.\n",
        "print('Original: ', seperated_tweets[154])\n",
        "print('Input IDs:', input_ids[154])\n",
        "print('Token IDs:', token_ids[154])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  [CLS] $MENTION$ $MENTION$ In case of antifa take a little bat or something. [SEP] Good for you- more of should do that kind of thing. [SEP]\n",
            "Input IDs: tensor([ 101, 1002, 5254, 1002, 1002, 5254, 1002, 1999, 2553, 1997, 3424, 7011,\n",
            "        2202, 1037, 2210, 7151, 2030, 2242, 1012,  102, 2204, 2005, 2017, 1011,\n",
            "        2062, 1997, 2323, 2079, 2008, 2785, 1997, 2518, 1012,  102,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
            "Token IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BndeVh9WEPKu",
        "colab_type": "text"
      },
      "source": [
        "## Attention Mask\n",
        "\n",
        "Since each tweet needs to be of the same lenght they have been padded to be of the same lenght as the longest tweet. BERT will not need to pay attention to the padding, thus attention masks are neeeded for each tweet to distingush between real input and padding. See more: https://huggingface.co/transformers/glossary.html#attention-mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltiOErEo3GaW",
        "colab_type": "text"
      },
      "source": [
        "## Split data into training and validation sets\n",
        "Guide also found here: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "Why it is good to have an validation set is because it is a good test to use for overfitting. Instead of just considering validation accuracy, validation loss can also be observed, which is basically a measurement of how confindent the model was on the correct prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXlwa9gX3N9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3586b412-f9e7-4e1a-807d-7b961f71029a"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, token_ids, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11,916 training samples\n",
            "1,324 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inBj4H0k4Ogk",
        "colab_type": "text"
      },
      "source": [
        "The guide also recommed to create an iterator for the dataset since this save on memory compared to using regular for loops. See more: https://pytorch.org/docs/stable/data.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6C2G8Uv4EWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32. 16 is choosen in this case because the GPU runs out of memory with 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFu-_dO14hv5",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvwsrALsekL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "45b5df7fe1534f16bb09bfd23f5fd463",
            "1bc3989d58b34ee9a337af4d24f2ed4f",
            "7be968f6943747c6b3f665393c17475b",
            "3cc77f3edd774f0ba5296e20d13b90e1",
            "43c7acecb56e480d8caa0b5fda3a5861",
            "6699b47f1a124319a88ba73deae1aff0",
            "f381d6626b8b47c29bc917ad27f46963",
            "c5a7a6737ecf45da8e792c706038ab2e",
            "2e292e28de0648998e88701946a7a662",
            "b4431cec230e4993bbffdd1c430e0d3b",
            "9f666e36622a41f1a8e43b0871f62f03",
            "97979412cc944b449923c696819cd5b6",
            "d79f9010e47440ef917a2fc2d8350172",
            "d57ef2afba8c4340801f73bff6882738",
            "2472a285dea7401784dc820c58321d31",
            "8b3044f70d65420394fe30da0972f25b"
          ]
        },
        "outputId": "356fb4c1-fe9c-4e8d-8ae4-7883b17e57cf"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139902599672160 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_dso8bfv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45b5df7fe1534f16bb09bfd23f5fd463",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:filelock:Lock 139902599672160 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:filelock:Lock 139902614146016 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "INFO:transformers.file_utils:https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpdgi1443z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e292e28de0648998e88701946a7a662",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:filelock:Lock 139902614146016 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3utxsTAfs78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7L2zqxIf3fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sNm47K1gKHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8UjvATRgSKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NSC6AGagddB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34fea197-8594-4056-ca11-ebf15ee3f30f"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_token_ids = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=b_token_ids, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_token_ids = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=b_token_ids, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    745.    Elapsed: 0:01:03.\n",
            "  Batch    80  of    745.    Elapsed: 0:02:05.\n",
            "  Batch   120  of    745.    Elapsed: 0:03:07.\n",
            "  Batch   160  of    745.    Elapsed: 0:04:10.\n",
            "  Batch   200  of    745.    Elapsed: 0:05:12.\n",
            "  Batch   240  of    745.    Elapsed: 0:06:14.\n",
            "  Batch   280  of    745.    Elapsed: 0:07:16.\n",
            "  Batch   320  of    745.    Elapsed: 0:08:19.\n",
            "  Batch   360  of    745.    Elapsed: 0:09:21.\n",
            "  Batch   400  of    745.    Elapsed: 0:10:23.\n",
            "  Batch   440  of    745.    Elapsed: 0:11:25.\n",
            "  Batch   480  of    745.    Elapsed: 0:12:28.\n",
            "  Batch   520  of    745.    Elapsed: 0:13:30.\n",
            "  Batch   560  of    745.    Elapsed: 0:14:32.\n",
            "  Batch   600  of    745.    Elapsed: 0:15:34.\n",
            "  Batch   640  of    745.    Elapsed: 0:16:36.\n",
            "  Batch   680  of    745.    Elapsed: 0:17:38.\n",
            "  Batch   720  of    745.    Elapsed: 0:18:40.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:19:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:48\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    745.    Elapsed: 0:01:02.\n",
            "  Batch    80  of    745.    Elapsed: 0:02:04.\n",
            "  Batch   120  of    745.    Elapsed: 0:03:07.\n",
            "  Batch   160  of    745.    Elapsed: 0:04:09.\n",
            "  Batch   200  of    745.    Elapsed: 0:05:11.\n",
            "  Batch   240  of    745.    Elapsed: 0:06:14.\n",
            "  Batch   280  of    745.    Elapsed: 0:07:16.\n",
            "  Batch   320  of    745.    Elapsed: 0:08:18.\n",
            "  Batch   360  of    745.    Elapsed: 0:09:20.\n",
            "  Batch   400  of    745.    Elapsed: 0:10:23.\n",
            "  Batch   440  of    745.    Elapsed: 0:11:25.\n",
            "  Batch   480  of    745.    Elapsed: 0:12:27.\n",
            "  Batch   520  of    745.    Elapsed: 0:13:30.\n",
            "  Batch   560  of    745.    Elapsed: 0:14:32.\n",
            "  Batch   600  of    745.    Elapsed: 0:15:34.\n",
            "  Batch   640  of    745.    Elapsed: 0:16:37.\n",
            "  Batch   680  of    745.    Elapsed: 0:17:39.\n",
            "  Batch   720  of    745.    Elapsed: 0:18:41.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epcoh took: 0:19:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:48\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    745.    Elapsed: 0:01:02.\n",
            "  Batch    80  of    745.    Elapsed: 0:02:05.\n",
            "  Batch   120  of    745.    Elapsed: 0:03:07.\n",
            "  Batch   160  of    745.    Elapsed: 0:04:09.\n",
            "  Batch   200  of    745.    Elapsed: 0:05:12.\n",
            "  Batch   240  of    745.    Elapsed: 0:06:14.\n",
            "  Batch   280  of    745.    Elapsed: 0:07:16.\n",
            "  Batch   320  of    745.    Elapsed: 0:08:19.\n",
            "  Batch   360  of    745.    Elapsed: 0:09:21.\n",
            "  Batch   400  of    745.    Elapsed: 0:10:23.\n",
            "  Batch   440  of    745.    Elapsed: 0:11:25.\n",
            "  Batch   480  of    745.    Elapsed: 0:12:28.\n",
            "  Batch   520  of    745.    Elapsed: 0:13:30.\n",
            "  Batch   560  of    745.    Elapsed: 0:14:32.\n",
            "  Batch   600  of    745.    Elapsed: 0:15:35.\n",
            "  Batch   640  of    745.    Elapsed: 0:16:37.\n",
            "  Batch   680  of    745.    Elapsed: 0:17:39.\n",
            "  Batch   720  of    745.    Elapsed: 0:18:42.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:19:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.52\n",
            "  Validation took: 0:00:48\n",
            "\n",
            "Training complete!\n",
            "Total training took 1:00:23 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41S2ndFXg-Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}