{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlMOBcBxGCfCMNuvTe7BqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josbex/HS-detection_in_social_media_posts/blob/master/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwoZWNNJ3Ml-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as  pd\n",
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpprsVDOFzee",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset needs to be loaded from the drive (this entails the dataset is in your drive). If so, just run the cell below and follow the link to get an authorization code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hU3QHh8TIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "f80d10e1-1c38-4227-da12-544b72862872"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHePnpgILWI2",
        "colab_type": "text"
      },
      "source": [
        "After the the dataset can be read from the drive, just specify the name of the dataset you want to read. For this case the OLID training dataset is loaded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pwUdqUHMRn-",
        "colab_type": "text"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15TIgCC8hsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "863acd9c-a39a-435b-bbbb-2af4202b3b98"
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/thesis/dataset/olid-training-v1.0.tsv\", sep=\"\\t\") \n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ftF8nAF4Qb",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing \n",
        "\n",
        "Some steps are needed for the data processing first the text preprocesser from https://github.com/cbaziotis/ekphrasis  is used for cleaning up the tweets from urls, users, hashtags and emoticons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U78oC1_kHQPj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "c34a4605-fd53-4b53-d441-03e5b6fb7135"
      },
      "source": [
        "!pip install ekphrasis\n",
        "!pip install tweet-preprocessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 51kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 61kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/1a/36ead6ae1bc3b82ea864ef87f8c8e1e06bec3a745dc65915f47b46f241d0/ujson-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (175kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82843 sha256=8de694512bfc99e058971c03443dd72837d84f22524c14c6721593b5d5f89740\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=072b14d6dda701574569209f114f085b4d2549a1ccc8e34857321e8b00185762\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.3 ekphrasis-0.5.1 ftfy-5.8 ujson-3.1.0\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQONDsV3VP_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "120ffd7b-d1f7-45f0-e962-bcbf036ee60f"
      },
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['email', 'percent', 'money', 'phone',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJUchtMDcFTu",
        "colab_type": "text"
      },
      "source": [
        "##Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQd2Tm05cCrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessor as p\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "def update_tweet(tweet, processed_tweet):\n",
        "  p.set_options(p.OPT.EMOJI)\n",
        "  #Remove any leftover emojiis\n",
        "  processed_tweet = p.clean(processed_tweet)\n",
        "  df.replace(tweet, processed_tweet, inplace=True)\n",
        "\n",
        "def tokenize_tweets():\n",
        "  for tweet in df.tweet:\n",
        "    update_tweet(tweet, tokens_to_string(text_processor.pre_process_doc(tweet)))\n",
        "\n",
        "def write_to_tsv(filename, tweets, labels):\n",
        "  with open('/content/gdrive/My Drive/thesis/dataset/' + filename + '.tsv', 'wt') as out_file:\n",
        "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "    tsv_writer.writerow(['tweet', 'label'])\n",
        "    for tweet, label in zip(tweets, labels):\n",
        "      tsv_writer.writerow([tweet, label])\n",
        "  out_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nguudp9LzIyC",
        "colab_type": "text"
      },
      "source": [
        "##Processing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKvwiazJabm3",
        "colab_type": "text"
      },
      "source": [
        "Example of before and after the tokenization. Hashtag segmetation is done using the twitter corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI7WXbm3aU86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "3feaea6b-d916-4aef-c470-20a64779b322"
      },
      "source": [
        "print(df.tweet[55])\n",
        "print(df.tweet[13239])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#GUNCONTROL advocates must STOP falling all over themselves to assure electorate that they too love the HORRIFIC 2A URL\n",
            "#Spanishrevenge vs. #justice #HumanRights and #FreedomOfExpression #Spain is a  #fakedemocracy @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER #cddr #shameonSpain #WakeupEurope @USER URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ1aueyQEfeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize_tweets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2by3ECe_kl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "39678fe9-445d-4b9e-8ea9-63d1d6ef4dc3"
      },
      "source": [
        "print(df.tweet[55])\n",
        "print(df.tweet[13239])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "guncontrol advocates must stop falling all over themselves to assure electorate that they too love the horrific 2 a url\n",
            "spanishrevenge vs . justice human rights and freedom of expression spain is a fake democracy @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user cd dr shameon spain wakeup europe @user url\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrBi-YDfP0t",
        "colab_type": "text"
      },
      "source": [
        "## Saving the labels and parsed tweets of the training data\n",
        "\n",
        "Saves the training data into numpy arrays. Labels are changed into binary representation where none offensive tweets are set to 0 and offensive is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NFvKl3N_4UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tweets = df.tweet.values\n",
        "#labels = df.subtask_a.values\n",
        "#labels = np.where(labels == \"NOT\", 0, 1)\n",
        "#write_to_tsv('training_data', tweets, labels)\n",
        "\n",
        "end = len(df) * 0.9\n",
        "train_df = df.iloc[0:int(end)]\n",
        "val_df = df.iloc[int(end):]\n",
        "\n",
        "#Save training data\n",
        "t_tweets = train_df.tweet.values\n",
        "t_labels = train_df.subtask_a.values\n",
        "t_labels = np.where(t_labels == \"NOT\", 0, 1)\n",
        "write_to_tsv('90_training_data', t_tweets, t_labels)\n",
        "\n",
        "#Save validation data\n",
        "v_tweets = val_df.tweet.values\n",
        "v_labels = val_df.subtask_a.values\n",
        "v_labels = np.where(v_labels == \"NOT\", 0, 1)\n",
        "write_to_tsv('10_val_data', v_tweets, v_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGMHisOYf9X-",
        "colab_type": "text"
      },
      "source": [
        "##Saving a random training and validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7pSwfTTfX8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.10, random_state=42)\n",
        "\n",
        "#Save training data\n",
        "t_tweets = train_df.tweet.values\n",
        "t_labels = train_df.subtask_a.values\n",
        "t_labels = np.where(t_labels == \"NOT\", 0, 1)\n",
        "write_to_tsv('90_random_training_data', t_tweets, t_labels)\n",
        "\n",
        "#Save validation data\n",
        "v_tweets = val_df.tweet.values\n",
        "v_labels = val_df.subtask_a.values\n",
        "v_labels = np.where(v_labels == \"NOT\", 0, 1)\n",
        "write_to_tsv('10_random_val_data', v_tweets, v_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCJmDK6AMV7U",
        "colab_type": "text"
      },
      "source": [
        "##Saving the labels and parsed tweets of the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG_9So-uylvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/thesis/dataset/testset-levela.tsv\", sep=\"\\t\") \n",
        "c_reader = csv.reader(open('/content/gdrive/My Drive/thesis/dataset/labels-levela.csv', 'r'), delimiter=',')\n",
        "tokenize_tweets()\n",
        "labels = [x[1] for x in c_reader]\n",
        "labels = np.array(labels)\n",
        "labels = np.where(labels == \"NOT\", 0, 1)\n",
        "tweets = df.tweet.values\n",
        "write_to_tsv('test_data', tweets, labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}