{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9UpLLBvonOBy5TO+hEd7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josbex/HS-detection_in_social_media_posts/blob/master/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwoZWNNJ3Ml-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as  pd\n",
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpprsVDOFzee",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset needs to be loaded from the drive (this entails the dataset is in your drive). If so, just run the cell below and follow the link to get an authorization code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hU3QHh8TIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "9d99bb03-d40c-4fe5-87b8-757c4000bec3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHePnpgILWI2",
        "colab_type": "text"
      },
      "source": [
        "After the the dataset can be read from the drive, just specify the name of the dataset you want to read. For this case the OLID training dataset is loaded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pwUdqUHMRn-",
        "colab_type": "text"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15TIgCC8hsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "c647ecaf-815a-4725-a839-695f828da1db"
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/thesis/dataset/olid-training-v1.0.tsv\", sep=\"\\t\") \n",
        "print(df.head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ftF8nAF4Qb",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing \n",
        "\n",
        "Some steps are needed for the data processing first the text preprocesser from https://github.com/cbaziotis/ekphrasis  is used for cleaning up the tweets from urls, users, hashtags and emoticons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U78oC1_kHQPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQONDsV3VP_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4188a81c-93a9-4755-fc78-ee21a5e3a3c5"
      },
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['email', 'percent', 'money', 'phone',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJUchtMDcFTu",
        "colab_type": "text"
      },
      "source": [
        "##Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQd2Tm05cCrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_string(tokens):\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "def update_tweet(tweet, processed_tweet):\n",
        "  df.replace(tweet, processed_tweet, inplace=True)\n",
        "\n",
        "def tokenize_tweets():\n",
        "  for tweet in df.tweet:\n",
        "    update_tweet(tweet, tokens_to_string(text_processor.pre_process_doc(tweet)))\n",
        "\n",
        "def write_to_tsv(filename, tweets, labels):\n",
        "  with open('/content/gdrive/My Drive/thesis/dataset/' + filename + '.tsv', 'wt') as out_file:\n",
        "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "    tsv_writer.writerow(['tweet', 'label'])\n",
        "    for tweet, label in zip(tweets, labels):\n",
        "      tsv_writer.writerow([tweet, label])\n",
        "  out_file.close()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nguudp9LzIyC",
        "colab_type": "text"
      },
      "source": [
        "##Processing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKvwiazJabm3",
        "colab_type": "text"
      },
      "source": [
        "Example of before and after the tokenization. Hashtag segmetation is done using the twitter corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI7WXbm3aU86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "37bb4f8d-b5cd-43e6-abec-9e7cf2832d6c"
      },
      "source": [
        "print(df.tweet[55])\n",
        "print(df.tweet[13239])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#GUNCONTROL advocates must STOP falling all over themselves to assure electorate that they too love the HORRIFIC 2A URL\n",
            "#Spanishrevenge vs. #justice #HumanRights and #FreedomOfExpression #Spain is a  #fakedemocracy @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER #cddr #shameonSpain #WakeupEurope @USER URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ1aueyQEfeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize_tweets()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2by3ECe_kl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "5a00da9e-9527-4e28-80ab-181c3cb2caca"
      },
      "source": [
        "print(df.tweet[55])\n",
        "print(df.tweet[13239])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "guncontrol advocates must stop falling all over themselves to assure electorate that they too love the horrific 2 a url\n",
            "spanishrevenge vs . justice human rights and freedom of expression spain is a fake democracy @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user cd dr shameon spain wakeup europe @user url\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrBi-YDfP0t",
        "colab_type": "text"
      },
      "source": [
        "## Saving the labels and parsed tweets of the training data\n",
        "\n",
        "Saves the training data into numpy arrays. Labels are changed into binary representation where none offensive tweets are set to 0 and offensive is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NFvKl3N_4UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df.tweet.values\n",
        "labels = df.subtask_a.values\n",
        "labels = np.where(labels == \"NOT\", 0, 1)\n",
        "write_to_tsv('training_data', tweets, labels)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCJmDK6AMV7U",
        "colab_type": "text"
      },
      "source": [
        "##Saving the labels and parsed tweets of the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG_9So-uylvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/thesis/dataset/testset-levela.tsv\", sep=\"\\t\") \n",
        "c_reader = csv.reader(open('/content/gdrive/My Drive/thesis/dataset/labels-levela.csv', 'r'), delimiter=',')\n",
        "tokenize_tweets()\n",
        "labels = [x[1] for x in c_reader]\n",
        "labels = np.array(labels)\n",
        "labels = np.where(labels == \"NOT\", 0, 1)\n",
        "tweets = df.tweet.values\n",
        "write_to_tsv('test_data', tweets, labels)"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}