{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo6uPl3daKgeEhUN9Hrabv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josbex/HS-detection_in_social_media_posts/blob/master/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwoZWNNJ3Ml-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as  pd\n",
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpprsVDOFzee",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset needs to be loaded from the drive (this entails the dataset is in your drive). If so, just run the cell below and follow the link to get an authorization code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hU3QHh8TIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2e5a36a-fd0a-4559-c99c-b5cb80624b99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHePnpgILWI2",
        "colab_type": "text"
      },
      "source": [
        "After the the dataset can be read from the drive, just specify the name of the dataset you want to read. For this case the OLID training dataset is loaded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pwUdqUHMRn-",
        "colab_type": "text"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15TIgCC8hsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "5dcbbdda-d9e2-47c3-d10d-21cace200c9f"
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/thesis/dataset/olid-training-v1.0.tsv\", sep=\"\\t\") \n",
        "print(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ftF8nAF4Qb",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing \n",
        "\n",
        "Some steps are needed for the data processing first the tweet-preprocesser (https://pypi.org/project/tweet-preprocessor/) is used for cleaning up the tweets from urls, users, hashtags and emoticons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IDlIdJ38pMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a692752-200d-4f9b-8322-784616439688"
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkqSH0lIZRWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessor as p"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V30IH1HWiZ7z",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize tweets\n",
        "\n",
        "Here we replace the @, emojis and urls using the tweet preprocessor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRP2zsl785EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_tweets():\n",
        "  #p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI)\n",
        "  p.set_options(p.OPT.EMOJI)\n",
        "  for tweet in df.tweet:\n",
        "    df.replace(tweet, p.tokenize(tweet), inplace=True)\n",
        "\n",
        "def remove_pattern(input_txt, pattern, replace):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, replace, input_txt)    \n",
        "    return input_txt  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8XQiE9GKNFp",
        "colab_type": "text"
      },
      "source": [
        "## Hashtag manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHMsmyUMA39S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/thesis')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8M4gSTYBEEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import vocab_parser as vp\n",
        "import hashtag_manipulation as hs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOgCkkgLKVEt",
        "colab_type": "text"
      },
      "source": [
        "## Vocab\n",
        "\n",
        "To be able to parse hashtags efficiently a big vocabulary is needed and just for this method the vocab needs to be in a list format sorted in word lenght order, shortest to longest. This will be updated later to be saved to an csv file, just to make it easier to add new words. \n",
        "\n",
        "As of now a list of 3000 most common english words was combined with a list of 1300 differents slurs and curse words. We can probably add some better vocab list later since this one doesn't work for different variations of words, for example it can split #humanright but not #humanrights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JGmmYTV9iUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "big_vocab = vp.file_to_list(\"/content/gdrive/My Drive/thesis/big_vocab.txt\")\n",
        "big_vocab.sort(key=len)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0B1z1MQA6UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = vp.file_to_list(\"/content/gdrive/My Drive/thesis/vocab.txt\")\n",
        "vocab.sort(key=len)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSb_kFnvKrVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "878a4f95-ef6b-4aac-c2cc-2c1a90ead775"
      },
      "source": [
        "print(vocab[0:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'a', 'I', 'ad', 'ah', 'AM', 'as', 'at', 'be', 'by']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPHzoGNX_b-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3773b314-2c1d-44e1-d3ce-be24deb7ffdb"
      },
      "source": [
        "print(hs.find_hashtags(df.tweet[13239]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#Spanishrevenge', '#justice', '#HumanRights', '#FreedomOfExpression', '#Spain', '#fakedemocracy', '#cddr', '#shameonSpain', '#WakeupEurope']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82m4BtAOFr6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reformat_hashtags():\n",
        "  for tweet in df.tweet:\n",
        "    new_tweet = tweet\n",
        "    hashtags = hs.find_hashtags(tweet)\n",
        "    if hashtags is not None:\n",
        "      for tag in hashtags:\n",
        "        c_tag = hs.clean_hashtag(str(tag))\n",
        "        split_hashtag = hs.split_tag(c_tag)\n",
        "        if len(split_hashtag) > 0:\n",
        "          new_tweet = hs.replace_hashtag(new_tweet, tag.strip(), hs.tag_to_string(split_hashtag))\n",
        "        else:\n",
        "          new_tweet = hs.replace_hashtag(new_tweet, tag.strip(), c_tag)\n",
        "    df.replace(tweet, new_tweet, inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPbidLih_fLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reformat_hashtags_vocab():\n",
        "  for tweet in df.tweet:\n",
        "    new_tweet = tweet\n",
        "    hashtags = hs.find_hashtags(tweet)\n",
        "    if hashtags is not None:\n",
        "      for tag in hashtags:\n",
        "        c_tag = hs.clean_hashtag(str(tag))\n",
        "        if len(c_tag) < 7:\n",
        "          split_hashtag = hs.split_tag(c_tag, vocab)\n",
        "          if len(split_hashtag) > 0:\n",
        "            new_tweet = hs.replace_hashtag(new_tweet, tag.strip(), hs.tag_to_string(split_hashtag))\n",
        "          else:\n",
        "            new_tweet = hs.replace_hashtag(new_tweet, tag.strip(), c_tag)\n",
        "        else:\n",
        "          new_tweet = hs.replace_hashtag(new_tweet, tag.strip(), c_tag)\n",
        "    df.replace(tweet, new_tweet, inplace=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nguudp9LzIyC",
        "colab_type": "text"
      },
      "source": [
        "##Processing the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ1aueyQEfeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize_tweets()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6JcYNZzBuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reformat_hashtags()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2by3ECe_kl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "14288d7c-6f5e-4068-a08b-3896b242e679"
      },
      "source": [
        "print(df.tweet[55])\n",
        "print(df.tweet[13239])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " gun control advocates must STOP falling all over themselves to assure electorate that they too love the HORRIFIC 2A URL\n",
            "spanishrevenge vs.  justice  human rights and  freedom of expression spain is a fakedemocracy @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER cddr shameonspain wakeupeurope @USER URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrBi-YDfP0t",
        "colab_type": "text"
      },
      "source": [
        "## Saving the labels and parsed tweets of the training data\n",
        "\n",
        "Saves the training data into numpy arrays. Labels are changed into binary representation where none offensive tweets are set to 0 and offensive is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4idvW_PfI3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_to_tsv(filename, tweets, labels):\n",
        "  with open('/content/gdrive/My Drive/thesis/dataset/' + filename + '.tsv', 'wt') as out_file:\n",
        "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "    tsv_writer.writerow(['tweet', 'label'])\n",
        "    for tweet, label in zip(tweets, labels):\n",
        "      tsv_writer.writerow([tweet, label])\n",
        "  out_file.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NFvKl3N_4UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df.tweet.values\n",
        "labels = df.subtask_a.values\n",
        "labels = np.where(labels == \"NOT\", 0, 1)\n",
        "write_to_tsv('training_data', tweets, labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCJmDK6AMV7U",
        "colab_type": "text"
      },
      "source": [
        "## Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3lYDz88LiXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/thesis/dataset/testset-levela.tsv\", sep=\"\\t\") \n",
        "c_reader = csv.reader(open('/content/gdrive/My Drive/thesis/dataset/labels-levela.csv', 'r'), delimiter=',')\n",
        "labels = [x[1] for x in c_reader]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6r9-dfhzY7D",
        "colab_type": "text"
      },
      "source": [
        "##Processing the test data\n",
        "\n",
        "Only the test data for sub_task_a of the OLID dataset is processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG_9So-uylvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize_tweets()\n",
        "reformat_hashtags()\n",
        "tweets = df.tweet.values\n",
        "write_to_tsv('test_data', tweets, labels)"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}